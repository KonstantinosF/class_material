{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Multi-layer Perceptron (with Embeddings)\n",
    "\n",
    "This notebook is an example of a multi-layer perceptron with Keras (https://keras.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import some needed packages\n",
    "import random\n",
    "random.seed(2019)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We are going to load the tweets from SemEval 2018..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "fpattern = '../Exercise_2-TwitterSentimentAnalysis/data/twitter-20*train-*.tsv'\n",
    "filenames = [filename for filename in sorted(glob.glob(fpattern))]\n",
    "# print(filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all files into a big data frame...\n",
    "column_names = ['id', 'tag', 'tweet']\n",
    "df = pd.concat([pd.read_csv(f, sep=\"\\t\", quoting=3, names=column_names) for f in filenames], ignore_index=True, sort=True)\n",
    "# df.info()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows having 'Not Available'...\n",
    "df = df[df.tweet != 'Not Available']\n",
    "# df.info()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A function to convert a tweet into a set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/petasis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# turn a document into a list of clean tokens\n",
    "def clean_doc(doc):\n",
    "    # Remove links...\n",
    "    doc = re.sub(\"\\w+:\\/\\/\\S+\", \" \", doc)\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process all tweets, and save results in the dataframe..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df['tokens'] = np.array([ clean_doc(tweet) for tweet in df.tweet ])\n",
    "# df.info()  \n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform all actions also for dev/test data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpattern = '../Exercise_2-TwitterSentimentAnalysis/data/twitter-20*dev-*.tsv'\n",
    "devfs    = [filename for filename in sorted(glob.glob(fpattern))]\n",
    "fpattern = '../Exercise_2-TwitterSentimentAnalysis/data/twitter-20*test-*.tsv'\n",
    "testfs   = [filename for filename in sorted(glob.glob(fpattern))]\n",
    "df_dev   = pd.concat([pd.read_csv(f, sep=\"\\t\", quoting=3, names=column_names) for f in devfs],  ignore_index=True, sort=True)\n",
    "df_test  = pd.concat([pd.read_csv(f, sep=\"\\t\", quoting=3, names=column_names) for f in testfs], ignore_index=True, sort=True)\n",
    "df_dev   = df_dev[df_dev.tweet != 'Not Available']\n",
    "df_test  = df_test[df_test.tweet != 'Not Available']\n",
    "df_dev['tokens']  = np.array([ clean_doc(tweet) for tweet in df_dev.tweet ])\n",
    "df_test['tokens'] = np.array([ clean_doc(tweet) for tweet in df_test.tweet ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract our vocabulary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets:  30790\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "vocabulary = Counter()\n",
    "for tweet_tokens in itertools.chain(df.tokens, df_dev.tokens, df_test.tokens):\n",
    "    vocabulary.update(tweet_tokens)\n",
    "\n",
    "print('Total tweets: ', sum(1 for _ in itertools.chain(df.tokens, df_dev.tokens, df_test.tokens)))\n",
    "# vocabulary.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter words using the vocabulary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_vector_words(tokens, vocabulary):\n",
    "    tokens = [w for w in tokens if w in vocabulary]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# print(df.tweet[0])\n",
    "# token_to_vector_words(df.tokens[0], vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vector_tokens']      = np.array([ token_to_vector_words(tweet, vocabulary) for tweet in df.tokens ])\n",
    "df_dev['vector_tokens']  = np.array([ token_to_vector_words(tweet, vocabulary) for tweet in df_dev.tokens ])\n",
    "df_test['vector_tokens'] = np.array([ token_to_vector_words(tweet, vocabulary) for tweet in df_test.tokens ])\n",
    "# df.info()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map tag from class (positive, negative) to numbers...\n",
    "df['btag']      = df.tag.astype('category').cat.codes\n",
    "df_dev['btag']  = df_dev.tag.astype('category').cat.codes\n",
    "df_test['btag'] = df_test.tag.astype('category').cat.codes\n",
    "# df_dev.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-trained embeddings...\n",
    "\n",
    "Pre-trained embeddings can be found:\n",
    "\n",
    "GloVe: http://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "Word2Vec: https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings into a dict...\n",
    "embedding_dimension = 50\n",
    "embeddings_index = {}\n",
    "glove_data = '../data/embeddings/glove.twitter.27B.'+str(embedding_dimension)+'d.txt'\n",
    "f = open(glove_data)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    value = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = value\n",
    "f.close()\n",
    " \n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets make our vectors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23740, 50)\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.51880002  0.039331    0.080883   ... -0.81913    -0.28933999\n",
      "   0.87558001]\n",
      " [ 0.43026     0.0081207  -0.0090224  ... -0.24276    -0.51657999\n",
      "   1.24720001]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.086368    1.26409996  0.18991999 ... -0.43009001 -0.30998001\n",
      "   0.047121  ]\n",
      " [-1.40139997  1.06110001 -0.14475    ... -0.16698    -0.26725999\n",
      "  -0.70081002]]\n",
      "[2625, 141, 358, 7, 6, 2340, 1249, 49]\n",
      "['Make', 'Sure', 'To', 'Come', 'To', 'The', 'Bob', 'Jones', 'Game', 'Friday', 'Free', 'Hot', 'Dogs', 'Hamburgers', 'amp', 'Food', 'outside', 'gate', 'amp', 'watch', 'Bob', 'Jones', 'take', 'Austin', 'High']\n",
      "[34, 119, 327, 29, 327, 5, 192, 1542, 15, 10, 96, 470, 1564, 14202, 11, 635, 1020, 3845, 11, 24, 192, 1542, 85, 1321, 465]\n",
      "Longest tweet (in words):  25\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df.vector_tokens)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# The embedding_matrix matrix maps words to vectors in the specified embedding dimension:\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dimension))\n",
    "for word, i in word_index.items():\n",
    "    # print(word, i) <= i starts from 1\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector;#[:embedding_dimension]\n",
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix)\n",
    "\n",
    "Xtrain = tokenizer.texts_to_sequences(df.vector_tokens)\n",
    "Ytrain = df.btag\n",
    "Xtest  = tokenizer.texts_to_sequences(df_test.vector_tokens)\n",
    "Ytest  = df_test.btag\n",
    "from keras.utils import to_categorical\n",
    "Ytrain_one_hot = to_categorical(Ytrain)\n",
    "Ytest_one_hot  = to_categorical(Ytest)\n",
    "print(Xtrain[0])\n",
    "\n",
    "## Get the longest tweet...\n",
    "longest = max(df.tokens,key=len)\n",
    "print(longest)\n",
    "longest = max(Xtrain,key=len)\n",
    "print(longest)\n",
    "longest = len(longest)\n",
    "print(\"Longest tweet (in words): \", longest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to set the length of all tweets to the longest tweet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2625, 141, 358, 7, 6, 2340, 1249, 49]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0 2625  141  358    7    6 2340 1249   49]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "print(Xtrain[0])\n",
    "Xtrain = pad_sequences(Xtrain, maxlen=longest)\n",
    "Xtest  = pad_sequences(Xtest,  maxlen=longest)\n",
    "print(Xtrain[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple MLP model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23740\n"
     ]
    }
   ],
   "source": [
    "n_words = embedding_matrix.shape[0]\n",
    "print(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/petasis/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 50)            1187000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1250)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 3753      \n",
      "=================================================================\n",
      "Total params: 1,190,753\n",
      "Trainable params: 3,753\n",
      "Non-trainable params: 1,187,000\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 179.00 264.00\" width=\"179pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-260 175,-260 175,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140151055051128 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140151055051128</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 171,-182.5 171,-146.5 0,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-160.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 140151057727104 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140151057727104</title>\n",
       "<polygon fill=\"none\" points=\"29,-73.5 29,-109.5 142,-109.5 142,-73.5 29,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-87.8\">flatten_1: Flatten</text>\n",
       "</g>\n",
       "<!-- 140151055051128&#45;&gt;140151057727104 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140151055051128-&gt;140151057727104</title>\n",
       "<path d=\"M85.5,-146.4551C85.5,-138.3828 85.5,-128.6764 85.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"89.0001,-119.5903 85.5,-109.5904 82.0001,-119.5904 89.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140151332666784 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140151332666784</title>\n",
       "<polygon fill=\"none\" points=\"32,-.5 32,-36.5 139,-36.5 139,-.5 32,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-14.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140151057727104&#45;&gt;140151332666784 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140151057727104-&gt;140151332666784</title>\n",
       "<path d=\"M85.5,-73.4551C85.5,-65.3828 85.5,-55.6764 85.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"89.0001,-46.5903 85.5,-36.5904 82.0001,-46.5904 89.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140151055062128 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140151055062128</title>\n",
       "<polygon fill=\"none\" points=\"21,-219.5 21,-255.5 150,-255.5 150,-219.5 21,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-233.8\">140151055062128</text>\n",
       "</g>\n",
       "<!-- 140151055062128&#45;&gt;140151055051128 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140151055062128-&gt;140151055051128</title>\n",
       "<path d=\"M85.5,-219.4551C85.5,-211.3828 85.5,-201.6764 85.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"89.0001,-192.5903 85.5,-182.5904 82.0001,-192.5904 89.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# define network\n",
    "model = Sequential()\n",
    "#model.add(Dense(units=64, activation='relu', input_shape=(n_words,)))\n",
    "model.add(Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    "    weights=[embedding_matrix], input_length=longest, trainable=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "# compile network\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n",
    "# summarize defined model\n",
    "model.summary()\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit our network..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/petasis/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/120\n",
      " - 0s - loss: 0.2216 - acc: 0.4331\n",
      "Epoch 2/120\n",
      " - 0s - loss: 0.2114 - acc: 0.4604\n",
      "Epoch 3/120\n",
      " - 0s - loss: 0.2057 - acc: 0.4830\n",
      "Epoch 4/120\n",
      " - 0s - loss: 0.2016 - acc: 0.4985\n",
      "Epoch 5/120\n",
      " - 0s - loss: 0.1983 - acc: 0.5128\n",
      "Epoch 6/120\n",
      " - 0s - loss: 0.1958 - acc: 0.5236\n",
      "Epoch 7/120\n",
      " - 0s - loss: 0.1935 - acc: 0.5328\n",
      "Epoch 8/120\n",
      " - 0s - loss: 0.1917 - acc: 0.5384\n",
      "Epoch 9/120\n",
      " - 0s - loss: 0.1901 - acc: 0.5450\n",
      "Epoch 10/120\n",
      " - 0s - loss: 0.1886 - acc: 0.5478\n",
      "Epoch 11/120\n",
      " - 0s - loss: 0.1873 - acc: 0.5540\n",
      "Epoch 12/120\n",
      " - 0s - loss: 0.1860 - acc: 0.5584\n",
      "Epoch 13/120\n",
      " - 0s - loss: 0.1849 - acc: 0.5613\n",
      "Epoch 14/120\n",
      " - 0s - loss: 0.1838 - acc: 0.5645\n",
      "Epoch 15/120\n",
      " - 0s - loss: 0.1828 - acc: 0.5662\n",
      "Epoch 16/120\n",
      " - 0s - loss: 0.1819 - acc: 0.5701\n",
      "Epoch 17/120\n",
      " - 0s - loss: 0.1810 - acc: 0.5723\n",
      "Epoch 18/120\n",
      " - 0s - loss: 0.1802 - acc: 0.5730\n",
      "Epoch 19/120\n",
      " - 0s - loss: 0.1794 - acc: 0.5770\n",
      "Epoch 20/120\n",
      " - 0s - loss: 0.1787 - acc: 0.5791\n",
      "Epoch 21/120\n",
      " - 0s - loss: 0.1780 - acc: 0.5823\n",
      "Epoch 22/120\n",
      " - 0s - loss: 0.1774 - acc: 0.5817\n",
      "Epoch 23/120\n",
      " - 0s - loss: 0.1767 - acc: 0.5842\n",
      "Epoch 24/120\n",
      " - 0s - loss: 0.1761 - acc: 0.5890\n",
      "Epoch 25/120\n",
      " - 0s - loss: 0.1755 - acc: 0.5913\n",
      "Epoch 26/120\n",
      " - 0s - loss: 0.1750 - acc: 0.5913\n",
      "Epoch 27/120\n",
      " - 0s - loss: 0.1745 - acc: 0.5944\n",
      "Epoch 28/120\n",
      " - 0s - loss: 0.1740 - acc: 0.5955\n",
      "Epoch 29/120\n",
      " - 0s - loss: 0.1736 - acc: 0.5980\n",
      "Epoch 30/120\n",
      " - 0s - loss: 0.1731 - acc: 0.5991\n",
      "Epoch 31/120\n",
      " - 0s - loss: 0.1727 - acc: 0.5990\n",
      "Epoch 32/120\n",
      " - 0s - loss: 0.1723 - acc: 0.6004\n",
      "Epoch 33/120\n",
      " - 0s - loss: 0.1719 - acc: 0.6036\n",
      "Epoch 34/120\n",
      " - 0s - loss: 0.1715 - acc: 0.6045\n",
      "Epoch 35/120\n",
      " - 0s - loss: 0.1711 - acc: 0.6059\n",
      "Epoch 36/120\n",
      " - 0s - loss: 0.1707 - acc: 0.6089\n",
      "Epoch 37/120\n",
      " - 0s - loss: 0.1704 - acc: 0.6079\n",
      "Epoch 38/120\n",
      " - 0s - loss: 0.1701 - acc: 0.6104\n",
      "Epoch 39/120\n",
      " - 0s - loss: 0.1698 - acc: 0.6096\n",
      "Epoch 40/120\n",
      " - 0s - loss: 0.1695 - acc: 0.6086\n",
      "Epoch 41/120\n",
      " - 0s - loss: 0.1692 - acc: 0.6111\n",
      "Epoch 42/120\n",
      " - 0s - loss: 0.1689 - acc: 0.6118\n",
      "Epoch 43/120\n",
      " - 0s - loss: 0.1685 - acc: 0.6118\n",
      "Epoch 44/120\n",
      " - 0s - loss: 0.1683 - acc: 0.6119\n",
      "Epoch 45/120\n",
      " - 0s - loss: 0.1680 - acc: 0.6147\n",
      "Epoch 46/120\n",
      " - 0s - loss: 0.1678 - acc: 0.6143\n",
      "Epoch 47/120\n",
      " - 0s - loss: 0.1675 - acc: 0.6143\n",
      "Epoch 48/120\n",
      " - 0s - loss: 0.1672 - acc: 0.6166\n",
      "Epoch 49/120\n",
      " - 0s - loss: 0.1670 - acc: 0.6162\n",
      "Epoch 50/120\n",
      " - 0s - loss: 0.1669 - acc: 0.6168\n",
      "Epoch 51/120\n",
      " - 0s - loss: 0.1666 - acc: 0.6179\n",
      "Epoch 52/120\n",
      " - 0s - loss: 0.1664 - acc: 0.6184\n",
      "Epoch 53/120\n",
      " - 0s - loss: 0.1662 - acc: 0.6187\n",
      "Epoch 54/120\n",
      " - 0s - loss: 0.1660 - acc: 0.6208\n",
      "Epoch 55/120\n",
      " - 0s - loss: 0.1658 - acc: 0.6194\n",
      "Epoch 56/120\n",
      " - 0s - loss: 0.1655 - acc: 0.6204\n",
      "Epoch 57/120\n",
      " - 0s - loss: 0.1654 - acc: 0.6213\n",
      "Epoch 58/120\n",
      " - 0s - loss: 0.1652 - acc: 0.6223\n",
      "Epoch 59/120\n",
      " - 0s - loss: 0.1650 - acc: 0.6240\n",
      "Epoch 60/120\n",
      " - 0s - loss: 0.1648 - acc: 0.6239\n",
      "Epoch 61/120\n",
      " - 0s - loss: 0.1647 - acc: 0.6236\n",
      "Epoch 62/120\n",
      " - 0s - loss: 0.1645 - acc: 0.6254\n",
      "Epoch 63/120\n",
      " - 0s - loss: 0.1644 - acc: 0.6243\n",
      "Epoch 64/120\n",
      " - 0s - loss: 0.1641 - acc: 0.6239\n",
      "Epoch 65/120\n",
      " - 0s - loss: 0.1640 - acc: 0.6223\n",
      "Epoch 66/120\n",
      " - 0s - loss: 0.1639 - acc: 0.6252\n",
      "Epoch 67/120\n",
      " - 0s - loss: 0.1637 - acc: 0.6259\n",
      "Epoch 68/120\n",
      " - 0s - loss: 0.1636 - acc: 0.6257\n",
      "Epoch 69/120\n",
      " - 0s - loss: 0.1634 - acc: 0.6262\n",
      "Epoch 70/120\n",
      " - 0s - loss: 0.1633 - acc: 0.6265\n",
      "Epoch 71/120\n",
      " - 0s - loss: 0.1631 - acc: 0.6266\n",
      "Epoch 72/120\n",
      " - 0s - loss: 0.1630 - acc: 0.6272\n",
      "Epoch 73/120\n",
      " - 0s - loss: 0.1628 - acc: 0.6281\n",
      "Epoch 74/120\n",
      " - 0s - loss: 0.1627 - acc: 0.6287\n",
      "Epoch 75/120\n",
      " - 0s - loss: 0.1626 - acc: 0.6281\n",
      "Epoch 76/120\n",
      " - 0s - loss: 0.1625 - acc: 0.6290\n",
      "Epoch 77/120\n",
      " - 0s - loss: 0.1623 - acc: 0.6272\n",
      "Epoch 78/120\n",
      " - 0s - loss: 0.1622 - acc: 0.6285\n",
      "Epoch 79/120\n",
      " - 0s - loss: 0.1621 - acc: 0.6293\n",
      "Epoch 80/120\n",
      " - 0s - loss: 0.1620 - acc: 0.6284\n",
      "Epoch 81/120\n",
      " - 0s - loss: 0.1618 - acc: 0.6298\n",
      "Epoch 82/120\n",
      " - 0s - loss: 0.1617 - acc: 0.6287\n",
      "Epoch 83/120\n",
      " - 0s - loss: 0.1616 - acc: 0.6306\n",
      "Epoch 84/120\n",
      " - 0s - loss: 0.1615 - acc: 0.6290\n",
      "Epoch 85/120\n",
      " - 0s - loss: 0.1614 - acc: 0.6310\n",
      "Epoch 86/120\n",
      " - 0s - loss: 0.1613 - acc: 0.6312\n",
      "Epoch 87/120\n",
      " - 0s - loss: 0.1612 - acc: 0.6309\n",
      "Epoch 88/120\n",
      " - 0s - loss: 0.1611 - acc: 0.6301\n",
      "Epoch 89/120\n",
      " - 0s - loss: 0.1610 - acc: 0.6307\n",
      "Epoch 90/120\n",
      " - 0s - loss: 0.1609 - acc: 0.6337\n",
      "Epoch 91/120\n",
      " - 0s - loss: 0.1608 - acc: 0.6322\n",
      "Epoch 92/120\n",
      " - 0s - loss: 0.1607 - acc: 0.6337\n",
      "Epoch 93/120\n",
      " - 0s - loss: 0.1606 - acc: 0.6311\n",
      "Epoch 94/120\n",
      " - 0s - loss: 0.1605 - acc: 0.6329\n",
      "Epoch 95/120\n",
      " - 0s - loss: 0.1604 - acc: 0.6342\n",
      "Epoch 96/120\n",
      " - 0s - loss: 0.1603 - acc: 0.6322\n",
      "Epoch 97/120\n",
      " - 0s - loss: 0.1602 - acc: 0.6338\n",
      "Epoch 98/120\n",
      " - 0s - loss: 0.1601 - acc: 0.6337\n",
      "Epoch 99/120\n",
      " - 0s - loss: 0.1600 - acc: 0.6322\n",
      "Epoch 100/120\n",
      " - 0s - loss: 0.1599 - acc: 0.6347\n",
      "Epoch 101/120\n",
      " - 0s - loss: 0.1599 - acc: 0.6346\n",
      "Epoch 102/120\n",
      " - 0s - loss: 0.1598 - acc: 0.6344\n",
      "Epoch 103/120\n",
      " - 0s - loss: 0.1597 - acc: 0.6359\n",
      "Epoch 104/120\n",
      " - 0s - loss: 0.1596 - acc: 0.6347\n",
      "Epoch 105/120\n",
      " - 0s - loss: 0.1596 - acc: 0.6371\n",
      "Epoch 106/120\n",
      " - 0s - loss: 0.1595 - acc: 0.6359\n",
      "Epoch 107/120\n",
      " - 0s - loss: 0.1594 - acc: 0.6342\n",
      "Epoch 108/120\n",
      " - 0s - loss: 0.1593 - acc: 0.6378\n",
      "Epoch 109/120\n",
      " - 0s - loss: 0.1592 - acc: 0.6358\n",
      "Epoch 110/120\n",
      " - 0s - loss: 0.1591 - acc: 0.6376\n",
      "Epoch 111/120\n",
      " - 0s - loss: 0.1591 - acc: 0.6370\n",
      "Epoch 112/120\n",
      " - 0s - loss: 0.1590 - acc: 0.6372\n",
      "Epoch 113/120\n",
      " - 0s - loss: 0.1589 - acc: 0.6375\n",
      "Epoch 114/120\n",
      " - 0s - loss: 0.1589 - acc: 0.6364\n",
      "Epoch 115/120\n",
      " - 0s - loss: 0.1588 - acc: 0.6387\n",
      "Epoch 116/120\n",
      " - 0s - loss: 0.1587 - acc: 0.6386\n",
      "Epoch 117/120\n",
      " - 0s - loss: 0.1587 - acc: 0.6388\n",
      "Epoch 118/120\n",
      " - 0s - loss: 0.1586 - acc: 0.6392\n",
      "Epoch 119/120\n",
      " - 0s - loss: 0.1586 - acc: 0.6396\n",
      "Epoch 120/120\n",
      " - 0s - loss: 0.1585 - acc: 0.6401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f77760089e8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit network\n",
    "from keras import backend as K\n",
    "model.fit(K.cast_to_floatx(Xtrain), K.cast_to_floatx(Ytrain_one_hot), batch_size=128, epochs=120, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our fit network...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 56.649044\n",
      "['negative', 'neutral', 'positive']\n",
      "[[ 750 1080  726]\n",
      " [ 665 4542 3171]\n",
      " [ 175 1645 4459]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.4717    0.2934    0.3618      2556\n",
      "     neutral     0.6250    0.5421    0.5806      8378\n",
      "    positive     0.5336    0.7101    0.6094      6279\n",
      "\n",
      "   micro avg     0.5665    0.5665    0.5665     17213\n",
      "   macro avg     0.5434    0.5152    0.5173     17213\n",
      "weighted avg     0.5689    0.5665    0.5586     17213\n",
      "\n",
      "0.5664904432696218\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "loss, acc = model.evaluate(K.cast_to_floatx(Xtest), K.cast_to_floatx(Ytest_one_hot), verbose=2)\n",
    "print('Test Accuracy: %f' % (acc*100))\n",
    "\n",
    "Y_hat = model.predict(K.cast_to_floatx(Xtest))\n",
    "Y_hat_int = np.argmax(Y_hat, axis=1)\n",
    "Y_int = np.argmax(Ytest_one_hot, axis=1)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "d = dict(enumerate(df_test.tag.astype('category').cat.categories))\n",
    "target_names = list(d.values())\n",
    "print(target_names)\n",
    "print(confusion_matrix(Y_int, Y_hat_int))\n",
    "print(classification_report(Y_int, Y_hat_int, digits=4, target_names=target_names))\n",
    "print(accuracy_score(Y_int, Y_hat_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add one more layer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 25, 50)            1187000   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1250)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 3753      \n",
      "=================================================================\n",
      "Total params: 1,190,753\n",
      "Trainable params: 1,190,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 179.00 264.00\" width=\"179pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-260 175,-260 175,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140151051834256 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140151051834256</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 171,-182.5 171,-146.5 0,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-160.8\">embedding_2: Embedding</text>\n",
       "</g>\n",
       "<!-- 140151053860648 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140151053860648</title>\n",
       "<polygon fill=\"none\" points=\"29,-73.5 29,-109.5 142,-109.5 142,-73.5 29,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-87.8\">flatten_2: Flatten</text>\n",
       "</g>\n",
       "<!-- 140151051834256&#45;&gt;140151053860648 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140151051834256-&gt;140151053860648</title>\n",
       "<path d=\"M85.5,-146.4551C85.5,-138.3828 85.5,-128.6764 85.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"89.0001,-119.5903 85.5,-109.5904 82.0001,-119.5904 89.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140151289235496 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140151289235496</title>\n",
       "<polygon fill=\"none\" points=\"32,-.5 32,-36.5 139,-36.5 139,-.5 32,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-14.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 140151053860648&#45;&gt;140151289235496 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140151053860648-&gt;140151289235496</title>\n",
       "<path d=\"M85.5,-73.4551C85.5,-65.3828 85.5,-55.6764 85.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"89.0001,-46.5903 85.5,-36.5904 82.0001,-46.5904 89.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140151051834760 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140151051834760</title>\n",
       "<polygon fill=\"none\" points=\"21,-219.5 21,-255.5 150,-255.5 150,-219.5 21,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-233.8\">140151051834760</text>\n",
       "</g>\n",
       "<!-- 140151051834760&#45;&gt;140151051834256 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140151051834760-&gt;140151051834256</title>\n",
       "<path d=\"M85.5,-219.4551C85.5,-211.3828 85.5,-201.6764 85.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"89.0001,-192.5903 85.5,-182.5904 82.0001,-192.5904 89.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras import optimizers\n",
    "\n",
    "# define network\n",
    "model = Sequential()\n",
    "#model.add(Dense(units=64, activation='relu', input_shape=(n_words,)))\n",
    "model.add(Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    "    weights=[embedding_matrix], input_length=longest, trainable=True))\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "# compile network\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "# summarize defined model\n",
    "model.summary()\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      " - 0s - loss: 1.1076 - acc: 0.4273\n",
      "Epoch 2/40\n",
      " - 0s - loss: 0.9777 - acc: 0.5062\n",
      "Epoch 3/40\n",
      " - 0s - loss: 0.9198 - acc: 0.5482\n",
      "Epoch 4/40\n",
      " - 0s - loss: 0.8871 - acc: 0.5718\n",
      "Epoch 5/40\n",
      " - 0s - loss: 0.8665 - acc: 0.5850\n",
      "Epoch 6/40\n",
      " - 0s - loss: 0.8529 - acc: 0.5917\n",
      "Epoch 7/40\n",
      " - 0s - loss: 0.8414 - acc: 0.6000\n",
      "Epoch 8/40\n",
      " - 0s - loss: 0.8335 - acc: 0.6066\n",
      "Epoch 9/40\n",
      " - 0s - loss: 0.8252 - acc: 0.6114\n",
      "Epoch 10/40\n",
      " - 0s - loss: 0.8193 - acc: 0.6134\n",
      "Epoch 11/40\n",
      " - 0s - loss: 0.8142 - acc: 0.6148\n",
      "Epoch 12/40\n",
      " - 0s - loss: 0.8096 - acc: 0.6195\n",
      "Epoch 13/40\n",
      " - 0s - loss: 0.8057 - acc: 0.6217\n",
      "Epoch 14/40\n",
      " - 0s - loss: 0.8021 - acc: 0.6213\n",
      "Epoch 15/40\n",
      " - 0s - loss: 0.7989 - acc: 0.6208\n",
      "Epoch 16/40\n",
      " - 0s - loss: 0.7960 - acc: 0.6226\n",
      "Epoch 17/40\n",
      " - 0s - loss: 0.7938 - acc: 0.6248\n",
      "Epoch 18/40\n",
      " - 0s - loss: 0.7915 - acc: 0.6282\n",
      "Epoch 19/40\n",
      " - 0s - loss: 0.7893 - acc: 0.6287\n",
      "Epoch 20/40\n",
      " - 0s - loss: 0.7870 - acc: 0.6282\n",
      "Epoch 21/40\n",
      " - 0s - loss: 0.7844 - acc: 0.6322\n",
      "Epoch 22/40\n",
      " - 0s - loss: 0.7828 - acc: 0.6323\n",
      "Epoch 23/40\n",
      " - 0s - loss: 0.7814 - acc: 0.6334\n",
      "Epoch 24/40\n",
      " - 0s - loss: 0.7799 - acc: 0.6337\n",
      "Epoch 25/40\n",
      " - 0s - loss: 0.7787 - acc: 0.6335\n",
      "Epoch 26/40\n",
      " - 0s - loss: 0.7772 - acc: 0.6356\n",
      "Epoch 27/40\n",
      " - 0s - loss: 0.7755 - acc: 0.6347\n",
      "Epoch 28/40\n",
      " - 0s - loss: 0.7744 - acc: 0.6383\n",
      "Epoch 29/40\n",
      " - 0s - loss: 0.7740 - acc: 0.6401\n",
      "Epoch 30/40\n",
      " - 0s - loss: 0.7728 - acc: 0.6372\n",
      "Epoch 31/40\n",
      " - 0s - loss: 0.7710 - acc: 0.6382\n",
      "Epoch 32/40\n",
      " - 0s - loss: 0.7701 - acc: 0.6400\n",
      "Epoch 33/40\n",
      " - 0s - loss: 0.7690 - acc: 0.6377\n",
      "Epoch 34/40\n",
      " - 0s - loss: 0.7679 - acc: 0.6391\n",
      "Epoch 35/40\n",
      " - 0s - loss: 0.7673 - acc: 0.6409\n",
      "Epoch 36/40\n",
      " - 0s - loss: 0.7665 - acc: 0.6416\n",
      "Epoch 37/40\n",
      " - 0s - loss: 0.7649 - acc: 0.6427\n",
      "Epoch 38/40\n",
      " - 0s - loss: 0.7641 - acc: 0.6414\n",
      "Epoch 39/40\n",
      " - 0s - loss: 0.7638 - acc: 0.6432\n",
      "Epoch 40/40\n",
      " - 0s - loss: 0.7628 - acc: 0.6436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7785843d30>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit network\n",
    "model.fit(K.cast_to_floatx(Xtrain), K.cast_to_floatx(Ytrain_one_hot), batch_size=1024, epochs=40, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 57.787719\n",
      "[[ 765 1232  559]\n",
      " [ 676 5128 2574]\n",
      " [ 200 2025 4054]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.4662    0.2993    0.3645      2556\n",
      "     neutral     0.6116    0.6121    0.6118      8378\n",
      "    positive     0.5641    0.6456    0.6021      6279\n",
      "\n",
      "   micro avg     0.5779    0.5779    0.5779     17213\n",
      "   macro avg     0.5473    0.5190    0.5262     17213\n",
      "weighted avg     0.5727    0.5779    0.5716     17213\n",
      "\n",
      "0.5778771858479057\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "loss, acc = model.evaluate(K.cast_to_floatx(Xtest), K.cast_to_floatx(Ytest_one_hot), verbose=2)\n",
    "print('Test Accuracy: %f' % (acc*100))\n",
    "\n",
    "Y_hat = model.predict(K.cast_to_floatx(Xtest))\n",
    "Y_hat_int = np.argmax(Y_hat, axis=1)\n",
    "Y_int = np.argmax(Ytest_one_hot, axis=1)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "print(confusion_matrix(Y_int, Y_hat_int))\n",
    "print(classification_report(Y_int, Y_hat_int, digits=4, target_names=target_names))\n",
    "print(accuracy_score(Y_int, Y_hat_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
